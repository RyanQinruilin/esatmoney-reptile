import urllib
from bs4 import BeautifulSoup
import re
import requests
from lxml import html
import xlwt
import time
import random
import chardet
######翻页函数########
def turnpage(Raw_url,pages):
    pro_lis = []
    for i in range(1,pages):
        fir_url = Raw_url.split('.')#http://finance.eastmoney.com/a/ccjdd.html
        pro_url = fir_url[0]+r'.'+fir_url[1]+r'.'+fir_url[2]+r'_'+str(i)+r'.'+fir_url[3]
        pro_lis.append(pro_url)
    return pro_lis
#############写入csv模块################
import csv
import os
import codecs
file_obj = codecs.open('gg3.csv','a+', 'utf-8') # 防止乱码
writer = csv.writer(file_obj)
writer.writerow(["标题","网址","内容","来源"])
##########构造网页###############
import requests
web_lis=[]
title_lis=[]
gg_webs = ['http://stock.eastmoney.com/news/cggdd.html']
for web in gg_webs:
    pro_webs = turnpage(web,4)
    for pro_web in pro_webs:
        web_lis.append(pro_web)
#############设置请求头##########
url = web_lis[0]
headers = { "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36"}
response = requests.post(url, headers=headers)
head = response.headers
response.encoding='utf-8'
#############获取网页##################
c=response.text
#print(c)
titles = re.findall('<p class="info" title="【(.*)】',c)
webs = re.findall('<a href="(.*2019[0-9]+.*.html)" target="_blank">',c)
for i in range(0,len(titles)):
    print(titles[i],webs[i])
    handle_web = requests.get(webs[i])
    soup= BeautifulSoup(handle_web.content,'lxml')
    for k in soup.find_all('div',class_ = 'newsContent'):
        a=''
        source =''
        for raw_content in k.find_all('p'):
            a =a+ raw_content.text
        for zb in k.find_all('div',class_='source data-source'):
            print(a,zb.text)
        writer.writerow([titles[i],webs[i],a,zb.text])
    i = i+1
