import urllib
from bs4 import BeautifulSoup
import re
import requests
from lxml import html
import xlwt
from xlutils.copy import copy as xl_copy
######翻页函数########
def turnpage(Raw_url,pages):
    pro_lis = []
    for i in range(1,pages):
        fir_url = Raw_url.split('.')#http://finance.eastmoney.com/a/ccjdd.html
        pro_url = fir_url[0]+r'.'+fir_url[1]+r'.'+fir_url[2]+r'_'+str(i)+r'.'+fir_url[3]
        pro_lis.append(pro_url)
    return pro_lis
def gp_content():
#############财经网页###########
    gp_lis = []
    title_lis = []
    web_lis = []
    content_lis = []
    pro_content=[]
    zb_lis = []
    gp_webs = ['http://stock.eastmoney.com/news/cscyw.html']
    for web in gp_webs:
        pro_lis = turnpage(web,3)
        for pro_web in pro_lis:
            gp_lis.append(pro_web)
#############获取网页的列表的内容##################
    for gp_web in gp_lis:
        handle_web = requests.get(gp_web)
        web_soup = BeautifulSoup(handle_web.content,'lxml')
        for titles in web_soup.find_all('div',class_='repeatList'):
            k = titles.find_all('a')
            for title in k:
                pro_webs = re.findall('<a href="(.*)" target="_blank">',str(title))
                for pro_web in pro_webs:
                    if pro_web not in web_lis:
                        web_lis.append(pro_web)
                pro_title = title.text
                if len(pro_title)<4:continue
                title_lis.append(pro_title)
                #######获取网页内容#####
                conhan_web = requests.get(pro_web)
                conweb_soup = BeautifulSoup(conhan_web.content,'lxml')
                raw_con = conweb_soup.find_all("div",class_='Body')
                for i in raw_con:
                    pro_con = i.find_all('p')
                    a=''
                    for content in pro_con:
                        a =a+ content.text
                        a.rstrip
                #######获取来源######
                    zb = re.findall("（文章来源：(.*)）",a)
                    for resou in zb:
                        zb_lis.append(resou)
                    print(pro_web,pro_title,a,resou) 
                pro_content.append(a)
    ####################写入excel表格############
    i = 1
    j = 0
    workbook = xlwt.Workbook(encoding = 'utf-8')
    worksheet = workbook.add_sheet('股票',cell_overwrite_ok=True)
    #for webs in all_web:
    worksheet.write(0, 1, label = '标题')
    worksheet.write(0, 2, label = '网址')
    worksheet.write(0, 3, label = '内容')
    worksheet.write(0, 4, label = '来源')
    for i in range(1,len(web_lis)+1):
        worksheet.write(i, 1, label = title_lis[i-1])
        worksheet.write(i, 2, label = web_lis[i-1])
        worksheet.write(i, 3, label = pro_content[i-1])
        worksheet.write(i, 4, label = zb_lis[i-1])
        #worksheet.write(i,j,label = str(webs))
        i = i+1
    #wb = xl_copy(workbook)
    # add sheet to workbook with existing sheets
    #Sheet1 = wb.add_sheet('财经')
    workbook.save('Excel_dfcfgp.xls')
def gp_content():
#############财经网页###########
    gp_lis = []
    title_lis = []
    web_lis = []
    content_lis = []
    pro_content=[]
    zb_lis = []
    gp_webs = ['http://stock.eastmoney.com/news/cscyw.html']
    for web in gp_webs:
        pro_lis = turnpage(web,3)
        for pro_web in pro_lis:
            gp_lis.append(pro_web)
#############获取网页的列表的内容##################
    for gp_web in gp_lis:
        handle_web = requests.get(gp_web)
        web_soup = BeautifulSoup(handle_web.content,'lxml')
        for titles in web_soup.find_all('div',class_='repeatList'):
            k = titles.find_all('a')
            for title in k:
                pro_webs = re.findall('<a href="(.*)" target="_blank">',str(title))
                for pro_web in pro_webs:
                    if pro_web not in web_lis:
                        web_lis.append(pro_web)
                pro_title = title.text
                if len(pro_title)<4:continue
                title_lis.append(pro_title)
                #######获取网页内容#####
                conhan_web = requests.get(pro_web)
                conweb_soup = BeautifulSoup(conhan_web.content,'lxml')
                raw_con = conweb_soup.find_all("div",class_='Body')
                for i in raw_con:
                    pro_con = i.find_all('p')
                    a=''
                    for content in pro_con:
                        a =a+ content.text
                        a.rstrip
                #######获取来源######
                    zb = re.findall("（文章来源：(.*)）",a)
                    for resou in zb:
                        zb_lis.append(resou)
                    print(pro_web,pro_title,a,resou) 
                pro_content.append(a)
    ####################写入excel表格############
    i = 1
    j = 0
    workbook = xlwt.Workbook(encoding = 'utf-8')
    worksheet = workbook.add_sheet('股票',cell_overwrite_ok=True)
    #for webs in all_web:
    worksheet.write(0, 1, label = '标题')
    worksheet.write(0, 2, label = '网址')
    worksheet.write(0, 3, label = '内容')
    worksheet.write(0, 4, label = '来源')
    for i in range(1,len(web_lis)+1):
        worksheet.write(i, 1, label = title_lis[i-1])
        worksheet.write(i, 2, label = web_lis[i-1])
        worksheet.write(i, 3, label = pro_content[i-1])
        worksheet.write(i, 4, label = zb_lis[i-1])
        #worksheet.write(i,j,label = str(webs))
        i = i+1
    #wb = xl_copy(workbook)
    # add sheet to workbook with existing sheets
    #Sheet1 = wb.add_sheet('财经')
    workbook.save('Excel_dfcfgp.xls')
    
gp_content()
