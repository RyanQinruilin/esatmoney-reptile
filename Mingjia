"""机构&名家"""
from lxml import html
import xml
import requests
from bs4 import BeautifulSoup
import numpy as np
import re
import urllib.request
#############名家p############
list_stock_allMing_web=[]
list_stock_allMingjia_web=[]
for main_bottom_right in main_bottom_right_lis:
    stock_sec_web= requests.get('http://mingjia.eastmoney.com/')
    stock_sec_soup = BeautifulSoup(stock_sec_web.content, "lxml") 
    #content = soup.find_all('div',class_="p12" )  
    for k in stock_sec_soup.find_all('div',class_='bottom'):
        Raw_sec_tweb = k.find_all('a')
        for Raw_secon_web in  Raw_sec_tweb:
            Raw_secon_web = str(Raw_secon_web)
            list_stock_allMing_web = re.findall('<a href="(.*)">.*</a>',Raw_secon_web )#正则筛选出网页
            #print(list_stock_allMingjia_web)
            for Raw_second_web in  list_stock_allMing_web:
                pro_second_web = r'http://mingjia.eastmoney.com'+str(Raw_second_web)
                list_stock_allMingjia_web.append(pro_second_web)
print(list_stock_allMingjia_web)
##########所有名家###############
list_stock_allMingjias_web=[]
for Mingjia_web in list_stock_allMingjia_web:
    Mingjia_sec_web= requests.get(Mingjia_web)
    Mingjia_sec_soup = BeautifulSoup(Mingjia_sec_web.content, "lxml")
    #print(Mingjia_sec_soup)
        #content = soup.find_all('div',class_="p12" ) 
    for k in Mingjia_sec_soup.find_all('div',class_='cmleft_bottom'):
        Raw_sec_tweb = k.find_all('a')
        for Raw_secon_web in  Raw_sec_tweb:
            Raw_secon_web = str(Raw_secon_web)
            list_Minjia_webs = re.findall('<a href="(.*)" target="_blank">',Raw_secon_web )
            #print(list_Minjia_web)
            for list_Minjia_web in list_Minjia_webs:
                pro_second_web = r'http://mingjia.eastmoney.com'+str(list_Minjia_web)
                list_stock_allMingjias_web.append(pro_second_web)
print(list_stock_allMingjias_web)
#############写入csv模块################
import csv
import os
import codecs
file_obj = codecs.open('mingjia.csv','a+', 'utf-8') # 防止乱码
writer = csv.writer(file_obj)
writer.writerow(["标题","网址","内容","来源"])
headers = { "User-Agent":"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)"}
#####名家的网址&页名###########
title_lis = []
title_web = []
for mingjiaurl in list_stock_allMingjias_web:
    for i in range(1,10):
            url = str(mingjiaurl)+r'/'+str(i)
        #try:
            print(url)
####获取内容######
            handle = requests.get(url,headers=headers)
            soup = BeautifulSoup(handle.content,'lxml')
            for title in soup.find_all('a',class_="acontent"):
                title_lis.append(title.text)
                web = re.findall('href="(.*)"',str(title))
                con_handle = requests.get(web[0])
                a = ''
                con_soup = BeautifulSoup(con_handle.content,'lxml')
                for k in con_soup.find_all('div',class_ = 'Body'):
                    cons = k.find_all('p')
                    for con in cons:
                        a = a+con.text
                    sourse = re.findall('（文章来源：（.*）',a)
                    print(web[0],title.text,a)
                    writer.writerow([web[0],title.text,a])
                title_web.append(web[0])
